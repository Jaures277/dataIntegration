import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum
import matplotlib.pyplot as plt

# ğŸ”§ Spark setup
os.environ["HADOOP_USER_NAME"] = "spark"
os.environ["USER"] = "spark"

spark = SparkSession.builder \
    .appName("Metrics_Calculation") \
    .config("spark.hadoop.fs.defaultFS", "hdfs://namenode:9000") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

# ğŸ“¥ Lecture des donnÃ©es depuis HDFS
df = spark.read.parquet("hdfs://namenode:9000/user/spark/output/v1")

# ğŸ§® Nombre total de prÃªts par Ã‰tat (filtrage des Ã©tats vides ou nuls)
df_pret_par_etat = df.filter(df.nan_state.isNotNull() & (df.nan_state != '')) \
    .groupBy("nan_state").agg(
        sum("subsidized_loans_originated_amount").alias("total_subsidized"),
        sum("unsubsidized_loans_originated_amount").alias("total_unsubsidized"),
        sum("parent_plus_loans_originated_amount").alias("total_parent_plus")
    )

# ğŸ’¬ Affichage pour capture terminal
print("ğŸ“Š RÃ©sumÃ© du nombre de prÃªts par Ã‰tat")
df_pret_par_etat.show(10, truncate=False)

# ğŸ“Š Conversion en Pandas pour tracÃ©
df_plot = df_pret_par_etat.toPandas()
df_plot["total"] = df_plot["total_subsidized"] + df_plot["total_unsubsidized"] + df_plot["total_parent_plus"]

# ğŸ“ˆ Courbe matplotlib
plt.figure(figsize=(12, 6))
df_plot.sort_values("total", ascending=False).plot(
    x="nan_state",
    y="total",
    kind="bar",
    legend=False,
    title="ğŸ“ˆ Nombre total de prÃªts par Ã‰tat"
)
plt.ylabel("Montant total des prÃªts ($)")
plt.tight_layout()
plt.savefig("/tmp/pret_par_etat.png")

# ğŸ“ Sauvegarde parquet facultative
df_pret_par_etat.write.mode("overwrite").parquet("hdfs://namenode:9000/user/spark/metrics/pret_par_etat")

print("âœ… MÃ©triques par Ã‰tat calculÃ©es et courbe gÃ©nÃ©rÃ©e : /tmp/pret_par_etat.png")
